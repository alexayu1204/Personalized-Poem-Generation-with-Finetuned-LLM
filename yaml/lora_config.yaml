model: "mlx-community/Qwen2.5-7B-Instruct-bf16"
train: true
fine_tune_type: lora
data: "./data"
seed: 0
num_layers: 8
batch_size: 4
iters: 1000
val_batches: 25
learning_rate: 1e-5
steps_per_report: 10
steps_per_eval: 200
resume_adapter_file: null
adapter_path: "adapters"
save_every: 100
test: false
test_batches: 100
max_seq_length: 2048
grad_checkpoint: false
lora_parameters:
    keys: ["self_attn.q_proj", "self_attn.v_proj", "mlp.down_proj", "mlp.gate_proj", "mlp.up_proj"]
    rank: 8
    alpha: 16
    dropout: 0.1
    scale: 16.0

lr_schedule:
    name: cosine_decay
    warmup: 100
    warmup_init: 1e-7
    arguments: [1e-5, 1000, 1e-7]
